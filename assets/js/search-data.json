{
  
    
        "post0": {
            "title": "Demystifying Entropy in ML üë®‚Äçüíª",
            "content": "This post is not my original work and inspired from the awesome video from Statquest. . Why are we discussing Entropy? . Entropy is used for a lot of purposes in Data Science. . It can be used to build classiification trees. | It is the basis for Mutual Information which quantified the relation between two things. | It is the basis of Relative Entropy(aka the Kullback-Leibler Distance) and Cross Entropy. | . All of the above use entropy or something derived from it to quantify similarities and differences. Let&#39;s learn how entropy quantifies similarities and differences . Utility Code . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings(&#39;ignore&#39;) from collections import Counter %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; . def plot_func(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = np.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . Introduction to Surprise &#128562;: Bags, Apples &amp; Oranges &#128188; &#127822; &#127818; . Let&#39;s say we have three bags üíº that are red, green, and blue in color. All these bags contain Apples üçé and Oranges üçä in different quantities. The task at hand is to quantify how similar or different these bags are from each other in terms of their composition using Entropy as a metric. . Since the number of apples in the red bag is more, the probability of picking up an apples from the red bag is higher. Hence it would not be very surprising, if we picked up an apples from the red bag. In contrast, if we picked up an orange from the red bag, we would be relatively more surprised. . red_bag = np.array([&#39;Apple&#39;]*4 + [&#39;Orange&#39;]) print(f&#39;Red Bag conatins: {Counter(red_bag)}&#39;) green_bag = np.array([&#39;Orange&#39;]*9 + [&#39;Apple&#39;]) print(f&#39;Green Bag conatins: {Counter(green_bag)}&#39;) white_bag = np.array([&#39;Orange&#39;]*5 + [&#39;Apple&#39;]*5) print(f&#39;White Bag conatins: {Counter(white_bag)}&#39;) . Red Bag conatins: Counter({&#39;Apple&#39;: 4, &#39;Orange&#39;: 1}) Green Bag conatins: Counter({&#39;Orange&#39;: 9, &#39;Apple&#39;: 1}) White Bag conatins: Counter({&#39;Orange&#39;: 5, &#39;Apple&#39;: 5}) . red_ctr = Counter(red_bag) P_apple_red = red_ctr[&#39;Apple&#39;] / len(red_bag) print(f&#39;P(Apple from Red Bag) : {round(P_apple_red,2)}&#39;) P_orange_red = red_ctr[&#39;Orange&#39;] / len(red_bag) print(f&#39;P(Orange from Red Bag): {round(P_orange_red,2)}&#39;) . P(Apple from Red Bag) : 0.8 P(Orange from Red Bag): 0.2 . The green bag has a lot more oranges than apples. Hence we would not be very surprised if we picked up an orange from this bag. On the other hand, if we picked up a apple, that would be relatively surprising. . green_ctr = Counter(green_bag) P_apple_green = green_ctr[&#39;Apple&#39;] / len(green_bag) print(f&#39;P(Apple from Green Bag) : {round(P_apple_green,2)}&#39;) P_orange_green = green_ctr[&#39;Orange&#39;] / len(green_bag) print(f&#39;P(Orange from Green Bag): {round(P_orange_green,2)}&#39;) . P(Apple from Green Bag) : 0.1 P(Orange from Green Bag): 0.9 . Since, the white bag has equal number of apples and oranges, we would be equally surprised, irrespective of whether we pick up and apple or orange. . white_ctr = Counter(white_bag) P_apple_white = white_ctr[&#39;Apple&#39;] / len(white_bag) print(f&#39;P(Apple from White Bag) : {round(P_apple_white,2)}&#39;) P_orange_white = white_ctr[&#39;Orange&#39;] / len(white_bag) print(f&#39;P(Orange from White Bag): {round(P_orange_white,2)}&#39;) . P(Apple from White Bag) : 0.5 P(Orange from White Bag): 0.5 . Looking at the above examples, we can observe the relationship between surprise and probability. . When the probability of picking an apple from the red bag is high, the surprise is low. | When the probability of picking an apple fronm the green bag is low, the surprise is high. | . Hence we can conclude the Surprise is inversely related to probability of an event. . Defining Surprise Mathematically &#129518; . Due to the inverse relationship between surprise and probability, we define surprise as the log of the inverse of the probability. Based on this definition, the surprise is zero when the probability is 1 and surprise is undefined when the probability is zero, which makes sense. We can see in the plot that probability increases surprise decreases and becomes zero when probability equals one. On the other hand, as probability nears zero, surprise tends to large values. . def surprise(probability): return np.log2(1/probability) # Ploting the plot_func(surprise, tx=&#39;Probabity&#39;, ty=&#39;Surprise&#39;, title=&#39;Surprise vs Entropy&#39;, min=0, max=1) . Let&#39;s assume that we have a biased coin with $P(H)$ = 0.9 and $P(T)$ = 0.1. We can calculate the surprise of heads and tails as follows . P_h = 0.9 P_t = 0.1 S_h = surprise(P_h); print(f&#39;Surprise of heads is: {round(S_h,2)}&#39;) S_t = surprise(P_t); print(f&#39;Surprise of tails is: {round(S_t,2)}&#39;) . Surprise of heads is: 0.15 Surprise of tails is: 3.32 . The surprise for tails is much larger than surprise of heads due to the inverse relationship. . Calculating surprise for a series of events . Let&#39;s calculate the surprise of getting 2 heads and 1 tail. It turns out that it is same adding the surprise of 2 heads and 1 tail as seen in the snippets below. . print(f&#39;Surprise by using the definition: {round(surprise(P_h * P_h * P_t),2)}&#39;) print(f&#39;Surprise by adding up the individual values: { round((S_h + S_h + S_t),2)} &#39;) . Surprise by using the definition: 3.63 Surprise by adding up the individual values: 3.63 . Entropy as the expected value of surprise . $$Entropy = sum frac{1}{log(p(x))} ast p(x)$$ . table = pd.DataFrame({&#39;Heads&#39;: [.15, .9], &#39;Tails&#39;: [3.32, .1]}, index=[&#39;S(x)&#39;,&#39;P(x)&#39;]); table . Heads Tails . S(x) 0.15 | 3.32 | . P(x) 0.90 | 0.10 | . Entropy = table.loc[&#39;P(x)&#39;,&#39;Heads&#39;] * table.loc[&#39;S(x)&#39;, &#39;Heads&#39;] + table.loc[&#39;P(x)&#39;, &#39;Tails&#39;] * table.loc[&#39;S(x)&#39;, &#39;Heads&#39;] Entropy . 0.15000000000000002 . From the above equation, it can be observed that the Entropy can also be represented as the dot product of the probability of observing certain events and their associated surprise. . P_e = np.array([.9, .1]) S_e = np.array([.15, 3.32]) Entropy = np.dot(P_e, S_e) Entropy . 0.467 . Entropy in Action &#127916; . def entropy(arr): ent = 0 probs = dict() ctr = Counter(arr) for e in ctr: probs[f&#39;P_{e}&#39;] = ctr[e] / len(arr) for p in probs: ent += -1 * probs[p] * np.log2(probs[p]) return round(ent, 2) . def show_entropy(bag_type=None, bag_type_str=None): ctr = Counter(bag_type) print(f&quot;{bag_type_str} conatins: {Counter(bag_type)}&quot;) print(f&quot;P(Apple from {bag_type_str}) : { ctr[&#39;Apple&#39;] / len(bag_type)}&quot;) print(f&quot;P(Orange from {bag_type_str}): {ctr[&#39;Orange&#39;] / len(bag_type)}&quot;) print(f&quot;Entropy of the {bag_type_str} is: {entropy(bag_type)}&quot;) . The entropy for the red bag turns out to be 0.72. It is closer to the probability of picking an apple from the red bag because there are more apples in the red bag as compared to oranges. . show_entropy(bag_type = red_bag, bag_type_str=&#39;Red Bag&#39;) . Red Bag conatins: Counter({&#39;Apple&#39;: 4, &#39;Orange&#39;: 1}) P(Apple from Red Bag) : 0.8 P(Orange from Red Bag): 0.2 Entropy of the Red Bag is: 0.72 . The entropy for the green bag turns out to be 0.47. This makes sense because green bag has a higher probability of picking a fruit with lower surprise. . show_entropy(bag_type = green_bag, bag_type_str=&#39;Green Bag&#39;) . Green Bag conatins: Counter({&#39;Orange&#39;: 9, &#39;Apple&#39;: 1}) P(Apple from Green Bag) : 0.1 P(Orange from Green Bag): 0.9 Entropy of the Green Bag is: 0.47 . The entropy for white bag is 1. In this case the probability and surprise have the same values for both the fruits. . show_entropy(bag_type = white_bag, bag_type_str=&#39;White Bag&#39;) . White Bag conatins: Counter({&#39;Orange&#39;: 5, &#39;Apple&#39;: 5}) P(Apple from White Bag) : 0.5 P(Orange from White Bag): 0.5 Entropy of the White Bag is: 1.0 . Summary . We use entropy to quantify the similarity or difference between two groups composed of a certain types of objects.Entropy of a group is highest when we have the same number of objects belonging to different classes in that group and it decreases as the difference between the objects of different classes increases. .",
            "url": "https://magicaditya.github.io/mlexplained/machine%20learning/python/2021/09/13/_09_06_entropy_in_data_science.html",
            "relUrl": "/machine%20learning/python/2021/09/13/_09_06_entropy_in_data_science.html",
            "date": " ‚Ä¢ Sep 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://magicaditya.github.io/mlexplained/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://magicaditya.github.io/mlexplained/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://magicaditya.github.io/mlexplained/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}